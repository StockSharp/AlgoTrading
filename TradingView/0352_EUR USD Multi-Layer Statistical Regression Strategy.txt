//@version=5
strategy("EUR/USD Multi-Layer Statistical Regression Strategy v2",
         overlay=true,
         initial_capital=100000,
         currency=currency.USD,
         default_qty_type=strategy.cash,
         default_qty_value=50000,
         commission_type=strategy.commission.percent,
         commission_value=0.0002,
         slippage=1,
         pyramiding=1)

// === MULTI-LAYER REGRESSION INPUTS ===
// Linear Regression Layers
lr_short_length = input.int(20, "Short-Term LR Length", minval=10, maxval=50, step=2)
lr_medium_length = input.int(50, "Medium-Term LR Length", minval=30, maxval=100, step=5)
lr_long_length = input.int(100, "Long-Term LR Length", minval=50, maxval=200, step=10)

// Statistical Validation - IMPROVED THRESHOLDS
min_r_squared = input.float(0.45, "Min R-Squared Threshold", minval=0.2, maxval=0.8, step=0.05)
slope_threshold = input.float(0.00005, "Min Slope Significance", minval=0.000001, maxval=0.0005, step=0.000005)
correlation_min = input.float(0.5, "Min Correlation", minval=0.3, maxval=0.8, step=0.05)

// Adaptive Statistical Thresholds
use_adaptive_thresholds = input.bool(true, "Use Adaptive Statistical Thresholds")
lookback_for_adaptation = input.int(100, "Adaptation Lookback Period", minval=50, maxval=200)

// Lookback/Look-Forward Analysis
validation_lookback = input.int(30, "Validation Lookback", minval=10, maxval=60, step=5)
prediction_horizon = input.int(10, "Prediction Horizon", minval=5, maxval=20, step=1)

// Dynamic Ensemble Weights
use_dynamic_weights = input.bool(true, "Use Dynamic Performance-Based Weights")
weight_short = input.float(0.4, "Base Short-Term Weight", minval=0.1, maxval=0.6, step=0.1)
weight_medium = input.float(0.35, "Base Medium-Term Weight", minval=0.1, maxval=0.6, step=0.05)
weight_long = input.float(0.25, "Base Long-Term Weight", minval=0.1, maxval=0.6, step=0.05)

// Risk Management
position_size_pct = input.float(50.0, "Position Size %", minval=10.0, maxval=100.0, step=5.0)
max_daily_loss = input.float(12.0, "Max Daily Loss %", minval=5.0, maxval=25.0, step=2.5)
confidence_threshold = input.float(0.65, "Signal Confidence Threshold", minval=0.5, maxval=0.85, step=0.05)

// === ENHANCED STATISTICAL FUNCTIONS ===
// Calculate Linear Regression with full statistics and error handling
linear_regression_stats(src, length) =>
    var float sum_x = 0
    var float sum_y = 0
    var float sum_xy = 0
    var float sum_x2 = 0
    var float sum_y2 = 0
    var int valid_points = 0

    // Reset sums
    sum_x := 0
    sum_y := 0
    sum_xy := 0
    sum_x2 := 0
    sum_y2 := 0
    valid_points := 0

    // Calculate sums for regression with NA handling
    for i = 0 to length - 1
        x = i + 1
        y = src[i]
        if not na(y)
            sum_x := sum_x + x
            sum_y := sum_y + y
            sum_xy := sum_xy + x * y
            sum_x2 := sum_x2 + x * x
            sum_y2 := sum_y2 + y * y
            valid_points := valid_points + 1

    n = valid_points

    // Ensure we have enough data points
    if n < math.max(5, length * 0.7)  // At least 70% valid data or minimum 5 points
        [na, 0.0, 0.0, 0.0, na]
    else
        // Calculate regression coefficients
        denominator = n * sum_x2 - sum_x * sum_x
        slope = denominator != 0 ? (n * sum_xy - sum_x * sum_y) / denominator : 0
        intercept = (sum_y - slope * sum_x) / n

        // Calculate correlation coefficient (R) with division by zero protection
        corr_denominator = math.sqrt((n * sum_x2 - sum_x * sum_x) * (n * sum_y2 - sum_y * sum_y))
        correlation = corr_denominator != 0 ? (n * sum_xy - sum_x * sum_y) / corr_denominator : 0

        // Calculate R-squared
        r_squared = correlation * correlation

        // Current regression value
        current_lr = intercept + slope * n

        // Projected value (look-forward)
        projected_lr = intercept + slope * (n + prediction_horizon)

        [current_lr, slope, r_squared, correlation, projected_lr]

// Adaptive statistical significance thresholds
calculate_adaptive_thresholds() =>
    var array<float> historical_r2_short = array.new<float>()
    var array<float> historical_r2_medium = array.new<float>()
    var array<float> historical_r2_long = array.new<float>()

    if use_adaptive_thresholds and bar_index >= lookback_for_adaptation
        // Store current RÂ² values for adaptation
        [_, _, r2_temp_short, _, _] = linear_regression_stats(close, lr_short_length)
        [_, _, r2_temp_medium, _, _] = linear_regression_stats(close, lr_medium_length)
        [_, _, r2_temp_long, _, _] = linear_regression_stats(close, lr_long_length)

        if not na(r2_temp_short)
            array.push(historical_r2_short, r2_temp_short)
            if array.size(historical_r2_short) > lookback_for_adaptation
                array.shift(historical_r2_short)

        if not na(r2_temp_medium)
            array.push(historical_r2_medium, r2_temp_medium)
            if array.size(historical_r2_medium) > lookback_for_adaptation
                array.shift(historical_r2_medium)

        if not na(r2_temp_long)
            array.push(historical_r2_long, r2_temp_long)
            if array.size(historical_r2_long) > lookback_for_adaptation
                array.shift(historical_r2_long)

        // Calculate adaptive thresholds (25th percentile to keep 75% of signals)
        adaptive_r2_threshold = array.size(historical_r2_short) > 20 ?
                               array.percentile_nearest_rank(historical_r2_short, 25) : min_r_squared

        math.max(0.2, math.min(0.7, adaptive_r2_threshold))  // Bound between 0.2 and 0.7
    else
        min_r_squared

adaptive_r2_threshold = calculate_adaptive_thresholds()

// Enhanced statistical significance test with fallback logic
enhanced_statistical_significance(r_squared, correlation, slope_abs, layer_name) =>
    // Primary significance test
    primary_significant = r_squared >= adaptive_r2_threshold and
                         math.abs(correlation) >= correlation_min and
                         math.abs(slope_abs) >= slope_threshold

    // Fallback test with relaxed criteria for critical short-term layer
    fallback_significant = layer_name == "short" ?
                          (r_squared >= 0.2 and math.abs(correlation) >= 0.3) :
                          (r_squared >= 0.3 and math.abs(correlation) >= 0.4)

    // Quality score for weighting even "insignificant" signals
    quality_score = (r_squared * 0.5 + math.abs(correlation) * 0.3 + math.min(1.0, math.abs(slope_abs) * 10000) * 0.2)

    [primary_significant, fallback_significant, quality_score]

// === MULTI-LAYER REGRESSION ANALYSIS ===
// Short-term layer
[lr_short, slope_short, r2_short, corr_short, proj_short] = linear_regression_stats(close, lr_short_length)
[sig_short_primary, sig_short_fallback, quality_short] = enhanced_statistical_significance(r2_short, corr_short, slope_short, "short")

// Medium-term layer
[lr_medium, slope_medium, r2_medium, corr_medium, proj_medium] = linear_regression_stats(close, lr_medium_length)
[sig_medium_primary, sig_medium_fallback, quality_medium] = enhanced_statistical_significance(r2_medium, corr_medium, slope_medium, "medium")

// Long-term layer
[lr_long, slope_long, r2_long, corr_long, proj_long] = linear_regression_stats(close, lr_long_length)
[sig_long_primary, sig_long_fallback, quality_long] = enhanced_statistical_significance(r2_long, corr_long, slope_long, "long")

// === DYNAMIC PERFORMANCE-BASED WEIGHTS ===
calculate_dynamic_weights() =>
    if use_dynamic_weights and bar_index >= 50
        // Base weights on recent performance quality
        total_quality = quality_short + quality_medium + quality_long

        if total_quality > 0
            dynamic_weight_short = (quality_short / total_quality) * 0.6 + weight_short * 0.4
            dynamic_weight_medium = (quality_medium / total_quality) * 0.6 + weight_medium * 0.4
            dynamic_weight_long = (quality_long / total_quality) * 0.6 + weight_long * 0.4

            // Normalize to ensure they sum to 1
            total_dynamic = dynamic_weight_short + dynamic_weight_medium + dynamic_weight_long
            [dynamic_weight_short/total_dynamic, dynamic_weight_medium/total_dynamic, dynamic_weight_long/total_dynamic]
        else
            [weight_short, weight_medium, weight_long]
    else
        [weight_short, weight_medium, weight_long]

[current_weight_short, current_weight_medium, current_weight_long] = calculate_dynamic_weights()

// === ENHANCED LOOKBACK VALIDATION ===
validate_prediction_accuracy() =>
    var array<float> accuracy_scores = array.new<float>()
    var array<float> short_errors = array.new<float>()
    var array<float> medium_errors = array.new<float>()
    var array<float> long_errors = array.new<float>()

    if bar_index >= validation_lookback + prediction_horizon
        // Multi-layer validation
        actual_change = (close - close[prediction_horizon]) / close[prediction_horizon]

        // Get historical predictions (avoid using potentially zero slopes)
        pred_slope_short = na(slope_short[prediction_horizon]) ? 0 : slope_short[prediction_horizon]
        pred_slope_medium = na(slope_medium[prediction_horizon]) ? 0 : slope_medium[prediction_horizon]
        pred_slope_long = na(slope_long[prediction_horizon]) ? 0 : slope_long[prediction_horizon]

        // Calculate prediction errors for each layer
        error_short = math.abs(actual_change - pred_slope_short * prediction_horizon)
        error_medium = math.abs(actual_change - pred_slope_medium * prediction_horizon)
        error_long = math.abs(actual_change - pred_slope_long * prediction_horizon)

        // Store errors for layer performance tracking
        array.push(short_errors, error_short)
        array.push(medium_errors, error_medium)
        array.push(long_errors, error_long)

        // Limit array sizes
        if array.size(short_errors) > validation_lookback
            array.shift(short_errors)
            array.shift(medium_errors)
            array.shift(long_errors)

        // Calculate ensemble prediction accuracy
        ensemble_pred = (pred_slope_short * current_weight_short + pred_slope_medium * current_weight_medium + pred_slope_long * current_weight_long) * prediction_horizon

        ensemble_error = math.abs(actual_change - ensemble_pred)
        accuracy = math.max(0, 1 - ensemble_error * 100)  // Scale appropriately

        array.push(accuracy_scores, accuracy)
        if array.size(accuracy_scores) > validation_lookback
            array.shift(accuracy_scores)

    // Return validation metrics
    avg_accuracy = array.size(accuracy_scores) > 5 ? array.avg(accuracy_scores) : 0.5
    short_reliability = array.size(short_errors) > 5 ? 1 - array.avg(short_errors) : 0.5

    [avg_accuracy, short_reliability]

[validation_accuracy, short_layer_reliability] = validate_prediction_accuracy()

// === ROBUST ENSEMBLE SIGNAL GENERATION ===
// Enhanced signal generation with quality weighting
generate_layer_signal(slope_val, r2_val, sig_primary, sig_fallback, quality_score) =>
    if sig_primary
        // High confidence signal
        slope_val > 0 ? 1.0 : -1.0
    else if sig_fallback
        // Medium confidence signal (reduced strength)
        (slope_val > 0 ? 0.6 : -0.6) * quality_score
    else if not na(slope_val) and quality_score > 0.3
        // Low confidence signal (heavily reduced)
        (slope_val > 0 ? 0.3 : -0.3) * quality_score
    else
        0.0

// Generate signals for each layer
signal_short = generate_layer_signal(slope_short, r2_short, sig_short_primary, sig_short_fallback, quality_short)
signal_medium = generate_layer_signal(slope_medium, r2_medium, sig_medium_primary, sig_medium_fallback, quality_medium)
signal_long = generate_layer_signal(slope_long, r2_long, sig_long_primary, sig_long_fallback, quality_long)

// Robust weighted ensemble score
ensemble_score = (signal_short * current_weight_short +
                  signal_medium * current_weight_medium +
                  signal_long * current_weight_long)

// Enhanced confidence calculation
calculate_overall_confidence() =>
    // Agreement score (how much layers agree)
    signals_array = array.from(signal_short, signal_medium, signal_long)
    non_zero_signals = 0
    signal_sum = 0.0

    for i = 0 to 2
        sig = array.get(signals_array, i)
        if math.abs(sig) > 0.1
            non_zero_signals += 1
            signal_sum += sig

    agreement_score = non_zero_signals > 0 ? math.abs(signal_sum) / non_zero_signals : 0

    // Statistical quality (weighted by current weights)
    stat_confidence = (r2_short * current_weight_short +
                      r2_medium * current_weight_medium +
                      r2_long * current_weight_long)

    // Validation confidence
    validation_confidence = validation_accuracy

    // Layer reliability (prevent over-reliance on unreliable layers)
    reliability_factor = math.min(1.0, short_layer_reliability + 0.3)  // Ensure short layer contributes

    // Overall confidence with reliability adjustment
    base_confidence = (agreement_score * 0.35 +
                      stat_confidence * 0.35 +
                      validation_confidence * 0.3)

    base_confidence * reliability_factor

overall_confidence = calculate_overall_confidence()

// === POSITION MANAGEMENT ===
// Enhanced trend analysis
trend_strength = math.abs(ensemble_score)
trend_direction = ensemble_score > 0.1 ? 1 : ensemble_score < -0.1 ? -1 : 0

// Adaptive position sizing
confidence_multiplier = overall_confidence > confidence_threshold ?
                       1.0 + (overall_confidence - confidence_threshold) * 0.5 :
                       overall_confidence / confidence_threshold * 0.8

base_position_value = strategy.equity * (position_size_pct / 100)
adjusted_position_value = base_position_value * confidence_multiplier

position_units = adjusted_position_value / close

// Daily loss tracking
var float daily_start_equity = strategy.equity
if ta.change(time("1D"))
    daily_start_equity := strategy.equity

current_daily_loss = daily_start_equity > 0 ? (daily_start_equity - strategy.equity) / daily_start_equity * 100 : 0
halt_trading = current_daily_loss > max_daily_loss

// === ENHANCED ENTRY/EXIT LOGIC ===
// More nuanced entry conditions
long_condition = ensemble_score > 0.3 and overall_confidence > confidence_threshold and (sig_short_primary or sig_short_fallback) and (sig_medium_primary or sig_medium_fallback)

short_condition = ensemble_score < -0.3 and overall_confidence > confidence_threshold and
                 (sig_short_primary or sig_short_fallback) and
                 (sig_medium_primary or sig_medium_fallback)

// Strategy execution
if long_condition and not halt_trading and strategy.position_size <= 0
    strategy.entry("Long", strategy.long, qty=position_units,
                   comment="Long: Conf=" + str.tostring(overall_confidence, "#.##") +
                          " ES=" + str.tostring(ensemble_score, "#.##"))

if short_condition and not halt_trading and strategy.position_size >= 0
    strategy.entry("Short", strategy.short, qty=position_units,
                   comment="Short: Conf=" + str.tostring(overall_confidence, "#.##") +
                          " ES=" + str.tostring(ensemble_score, "#.##"))

// Initialize with relaxed conditions
if strategy.position_size == 0 and not halt_trading and barstate.isconfirmed
    if ensemble_score > 0.2 and overall_confidence > 0.5
        strategy.entry("InitLong", strategy.long, qty=position_units, comment="Init Long")
    else if ensemble_score < -0.2 and overall_confidence > 0.5
        strategy.entry("InitShort", strategy.short, qty=position_units, comment="Init Short")

// Emergency exit
if halt_trading and strategy.position_size != 0
    strategy.close_all(comment="Daily Loss Limit")

// === VISUALIZATION ===
// Plot regression lines with enhanced styling
plot(lr_short, "Short-Term LR", color=sig_short_primary ? color.blue : color.new(color.blue, 50), linewidth=2)
plot(lr_medium, "Medium-Term LR", color=sig_medium_primary ? color.orange : color.new(color.orange, 50), linewidth=2)
plot(lr_long, "Long-Term LR", color=sig_long_primary ? color.purple : color.new(color.purple, 50), linewidth=2)

// Enhanced background coloring
bgcolor(overall_confidence > confidence_threshold ? (ensemble_score > 0.3 ? color.new(color.green, 90) : ensemble_score < -0.3 ? color.new(color.red, 90) : color.new(color.gray, 95)) : color.new(color.gray, 95))

// Signal markers
plotshape(long_condition, "Long Signal", shape.triangleup, location.belowbar,
          color=color.green, size=size.normal)
plotshape(short_condition, "Short Signal", shape.triangledown, location.abovebar,
          color=color.red, size=size.normal)

// === ENHANCED STATISTICS DASHBOARD ===
var table stats_table = table.new(position.top_right, 4, 12,
                                  bgcolor=color.white, border_width=1,
                                  frame_color=color.black, frame_width=1)

if barstate.islast
    // Clear table
    table.clear(stats_table, 0, 0, 3, 11)

    // Headers
    table.cell(stats_table, 0, 0, "Metric", text_color=color.black, bgcolor=color.gray, text_size=size.small)
    table.cell(stats_table, 1, 0, "Value", text_color=color.black, bgcolor=color.gray, text_size=size.small)
    table.cell(stats_table, 2, 0, "Quality", text_color=color.black, bgcolor=color.gray, text_size=size.small)
    table.cell(stats_table, 3, 0, "Weight", text_color=color.black, bgcolor=color.gray, text_size=size.small)

    // Performance metrics
    table.cell(stats_table, 0, 1, "Net Profit", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 1, "$" + str.tostring(strategy.netprofit, "#,###"),
               text_color=strategy.netprofit > 0 ? color.green : color.red, text_size=size.small)

    // RÂ² values with fallback status
    table.cell(stats_table, 0, 2, "Short RÂ²", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 2, str.tostring(r2_short, "#.##"),
               text_color=sig_short_primary ? color.green : sig_short_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 2, 2, sig_short_primary ? "â" : sig_short_fallback ? "~" : "â",
               text_color=sig_short_primary ? color.green : sig_short_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 3, 2, str.tostring(current_weight_short, "#.##"),
               text_color=color.blue, text_size=size.small)

    table.cell(stats_table, 0, 3, "Medium RÂ²", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 3, str.tostring(r2_medium, "#.##"),
               text_color=sig_medium_primary ? color.green : sig_medium_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 2, 3, sig_medium_primary ? "â" : sig_medium_fallback ? "~" : "â",
               text_color=sig_medium_primary ? color.green : sig_medium_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 3, 3, str.tostring(current_weight_medium, "#.##"),
               text_color=color.orange, text_size=size.small)

    table.cell(stats_table, 0, 4, "Long RÂ²", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 4, str.tostring(r2_long, "#.##"),
               text_color=sig_long_primary ? color.green : sig_long_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 2, 4, sig_long_primary ? "â" : sig_long_fallback ? "~" : "â",
               text_color=sig_long_primary ? color.green : sig_long_fallback ? color.orange : color.red, text_size=size.small)
    table.cell(stats_table, 3, 4, str.tostring(current_weight_long, "#.##"),
               text_color=color.purple, text_size=size.small)

    // Signal strengths
    table.cell(stats_table, 0, 5, "Short Signal", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 5, str.tostring(signal_short, "#.##"),
               text_color=signal_short > 0 ? color.green : signal_short < 0 ? color.red : color.gray, text_size=size.small)

    table.cell(stats_table, 0, 6, "Medium Signal", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 6, str.tostring(signal_medium, "#.##"),
               text_color=signal_medium > 0 ? color.green : signal_medium < 0 ? color.red : color.gray, text_size=size.small)

    table.cell(stats_table, 0, 7, "Long Signal", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 7, str.tostring(signal_long, "#.##"),
               text_color=signal_long > 0 ? color.green : signal_long < 0 ? color.red : color.gray, text_size=size.small)

    // Overall metrics
    table.cell(stats_table, 0, 8, "Ensemble", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 8, str.tostring(ensemble_score, "#.##"),
               text_color=ensemble_score > 0.3 ? color.green : ensemble_score < -0.3 ? color.red : color.gray, text_size=size.small)
    table.cell(stats_table, 2, 8, trend_direction > 0 ? "LONG" : trend_direction < 0 ? "SHORT" : "NEUTRAL",
               text_color=trend_direction > 0 ? color.green : trend_direction < 0 ? color.red : color.gray, text_size=size.small)

    table.cell(stats_table, 0, 9, "Confidence", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 9, str.tostring(overall_confidence, "#.##"),
               text_color=overall_confidence > confidence_threshold ? color.green : color.red, text_size=size.small)

    table.cell(stats_table, 0, 10, "Adapt RÂ² Thresh", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 10, str.tostring(adaptive_r2_threshold, "#.##"),
               text_color=color.blue, text_size=size.small)

    table.cell(stats_table, 0, 11, "Short Reliability", text_color=color.black, text_size=size.small)
    table.cell(stats_table, 1, 11, str.tostring(short_layer_reliability, "#.##"),
               text_color=short_layer_reliability > 0.5 ? color.green : color.red, text_size=size.small)

// Debug plots
plot(overall_confidence, "Overall Confidence", color=color.purple, display=display.data_window)
plot(ensemble_score, "Ensemble Score", color=color.blue, display=display.data_window)
plot(short_layer_reliability, "Short Layer Reliability", color=color.yellow, display=display.data_window)